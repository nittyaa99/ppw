
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Nama : Isnita Widyur Rahmah &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Tugas4/Gambar/22-048_Isnita Widyur Rahmah_Skip-Gram';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
        
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="My sample book - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="My sample book - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Welcome to your Jupyter Book
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../markdown.html">Markdown Files</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../notebooks.html">Content with notebooks</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../markdown-notebooks.html">Notebooks with MyST Markdown</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tugas2/vsm.html"><strong>PENCARIAN DAN PENAMBANGAN WEB - TUGAS 2 : Membuat VSM dan Menghitung TF-IDF berdasarkan hasil crawling berita</strong></a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FTugas4/Gambar/22-048_Isnita Widyur Rahmah_Skip-Gram.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/Tugas4/Gambar/22-048_Isnita Widyur Rahmah_Skip-Gram.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Nama : Isnita Widyur Rahmah</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Nama : Isnita Widyur Rahmah</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#nim-220411100048">NIM : 220411100048</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#kelas-if-7a">Kelas : IF 7A</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embedding">Word Embedding</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skip-gram">Skip-Gram</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#representasi-vektor-kata">1. Representasi Vektor Kata</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#contoh-analogi-dengan-vektor-kata">Contoh Analogi dengan Vektor Kata:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#konsep-window-size-dalam-model-skip-gram">2. Konsep Window Size dalam model Skip-Gram</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#struktur-skip-gram">3. Struktur Skip-Gram</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation-dalam-skip-gram">3.1 Forward Propagation dalam Skip-Gram</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matriks-embedding-skip-gram">3.2 Matriks Embedding Skip-Gram</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hidden-layer-skip-gram">3.3 Hidden Layer Skip-Gram</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-output-softmax-dalam-model-skip-gram">3.4 Layer Output Softmax dalam Model Skip-Gram</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-propagation-dalam-model-skip-gram">3.5 Backward Propagation dalam Model Skip-Gram</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-demonstration">4. Numerical demonstration</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#menghitung-hidden-projection-layer-dalam-model-skip-gram">4.1 Menghitung Hidden (Projection) Layer dalam Model Skip-Gram</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-output-layer-dalam-model-skip-gram">4.2 Softmax Output Layer dalam Model Skip-Gram</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#menjumlahkan-prediction-error-dari-kata-konteks">4.3 Menjumlahkan prediction error dari kata konteks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#perhitungan-gradien-untuk-matriks-bobot-input-input">4.4 Perhitungan Gradien untuk Matriks Bobot Input (âˆ‡ğ‘Šinput)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#perhitungan-gradien-untuk-matriks-bobot-output-output">4.5 perhitungan gradien untuk matriks bobot output (âˆ‡ğ‘Šoutput)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pembaruan-matriks-bobot-weight-matrices-melalui-backward-propagation">4.6 Pembaruan Matriks Bobot (Weight Matrices) melalui Backward Propagation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perhitungan-manual-skip-gram">Perhitungan Manual Skip-Gram</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#contoh-kata-ibu-beli-sayur-di-pasar">Contoh kata = ibu beli sayur di pasar</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#iterasi-1">Iterasi-1</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#iterasi-2">Iterasi 2</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-perhitungan">Code Perhitungan</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="nama-isnita-widyur-rahmah">
<h1>Nama : Isnita Widyur Rahmah<a class="headerlink" href="#nama-isnita-widyur-rahmah" title="Link to this heading">#</a></h1>
</section>
<section id="nim-220411100048">
<h1>NIM : 220411100048<a class="headerlink" href="#nim-220411100048" title="Link to this heading">#</a></h1>
</section>
<section id="kelas-if-7a">
<h1>Kelas : IF 7A<a class="headerlink" href="#kelas-if-7a" title="Link to this heading">#</a></h1>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="word-embedding">
<h1>Word Embedding<a class="headerlink" href="#word-embedding" title="Link to this heading">#</a></h1>
<p>Word embedding adalah teknik yang mengubah kata-kata menjadi representasi vektor numerik, sehingga kata-kata yang memiliki makna atau konteks serupa akan memiliki vektor yang dekat satu sama lain. Tujuan utamanya adalah untuk mengubah kata-kata (yang berupa teks) menjadi angka sehingga dapat diproses oleh model pembelajaran mesin (machine learning) atau pembelajaran mendalam (deep learning).</p>
<section id="skip-gram">
<h2>Skip-Gram<a class="headerlink" href="#skip-gram" title="Link to this heading">#</a></h2>
<p>Skip-gram adalah metode yang memprediksi kata-kata konteks di sekitar kata target untuk menghasilkan representasi vektor kata dalam model Word2Vec. Kata-kata konteks ini bisa terletak di sebelah kiri atau kanan dari kata target, tergantung pada ukuran jendela (window size) yang telah ditentukan. Misalnya, jika memilih kata target w(t), model akan berusaha memprediksi kata-kata konteks di sekitar kata tersebut, seperti w(tâˆ’2), w(tâˆ’1), w(t+1), w(t+2).
<img alt="1" src="https://hackmd.io/_uploads/HJ88W_rRA.png" /></p>
<section id="representasi-vektor-kata">
<h3>1. Representasi Vektor Kata<a class="headerlink" href="#representasi-vektor-kata" title="Link to this heading">#</a></h3>
<p><img alt="2" src="https://hackmd.io/_uploads/BkZabuBA0.png" />
Korelasi antara dua kata, seperti â€œsuccessâ€ dan â€œachieveâ€, bisa dihitung menggunakan jarak vektor di antara keduanya. Dalam visualisasi tiga dimensi, jarak ini menunjukkan seberapa dekat atau jauh makna kedua kata. Biasanya, jarak Cosine digunakan untuk mengukur hubungan antara vektor, meskipun dalam contoh visualisasi ini jarak Euclidean yang digunakan.</p>
<section id="contoh-analogi-dengan-vektor-kata">
<h4>Contoh Analogi dengan Vektor Kata:<a class="headerlink" href="#contoh-analogi-dengan-vektor-kata" title="Link to this heading">#</a></h4>
<p>Misalnya, untuk mengetahui ibu kota Jerman, dapat dilakukan dengan menggunakan operasi vektor seperti berikut
\begin{align*}
vec(\text{Germany}) &amp; = [1.22 \quad 0.34 \quad -3.82] \
vec(\text{capital}) &amp; = [3.02 \quad -0.93 \quad 1.82] \
vec(\text{Berlin})  &amp; = [4.09 \quad -0.58 \quad 2.01]
\end{align*}Dengan menambahkan vektor kata Germany dan capital
\begin{align*}
vec(\text{Germany}) + vec(\text{capital}) &amp;= [1.22 \quad 0.34 \quad -3.82] + [3.02 \quad -0.93 \quad 1.82] \
&amp;= [4.24 \quad -0.59 \quad -2.00]
\end{align*}Hasil ini hampir sama dengan vektor kata Berlin, sehingga model dapat menyimpulkan bahwa ibu kota Jerman adalah Berlin.</p>
</section>
</section>
<section id="konsep-window-size-dalam-model-skip-gram">
<h3>2. Konsep Window Size dalam model Skip-Gram<a class="headerlink" href="#konsep-window-size-dalam-model-skip-gram" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p>Softmax Regression
Softmax regression (atau multinomial logistic regression) adalah generalisasi dari logistic regression yang digunakan ketika ada lebih dari dua kelas yang ingin diklasifikasikan.
Rumus umum untuk softmax regression adalah:
$<span class="math notranslate nohighlight">\(J(\theta) = -\frac{1}{T} \sum^T_{t=1}\sum^K_{k=1}log \frac{exp(\theta^{(k)\top}x^{(t)})}{\sum^K_{i=1}exp(\theta^{(i)\top}x^{(t)})} \tag{10}\)</span>$T adalah jumlah sampel pelatihan.
K adalah jumlah label yang harus diklasifikasikan.
Dalam aplikasi Natural Language Processing (NLP),  ğ¾ diatur sama dengan ğ‘‰, yaitu jumlah kata dalam kosakata. Kosakata ini bisa sangat besar, bahkan mencapai puluhan ribu kata.</p></li>
<li><p>Penerapan dalam Skip-Gram
Model Skip-Gram mengadaptasi rumus softmax dengan mengganti ğ¾ dengan window size ğ¶. Window size adalah parameter hiper yang biasanya berkisar antara 1 hingga 10. Ini merepresentasikan jumlah kata konteks yang akan diprediksi di sekitar kata target. Skip-Gram tidak perlu memprediksi semua ğ‘‰ kata dalam korpus yang mungkin berjarak ratusan kata dari kata target. Sebaliknya, ia hanya fokus pada memprediksi 1 hingga 10 kata konteks terdekat.</p></li>
<li><p>Rumus yang diadaptasi untuk Skip-Gram
$<span class="math notranslate nohighlight">\(J(\theta) = -\frac{1}{T} \sum^T_{t=1}\sum_{c\leq j \leq c,j\neq 0}log\frac{exp(\theta^{(t+j)\top}x^{(t)})}{\sum^K_{i=1}exp(\theta^{(i)\top}x^{(t)})} \tag{11}\)</span>$ğ‘— menunjukkan posisi relatif kata konteks terhadap kata target. Nilai  ğ‘— dapat positif atau negatif untuk menandakan posisi kiri atau kanan.
ğ¾ dalam penyebut merujuk pada jumlah total kata dalam kosakata ğ‘‰. Ini memastikan probabilitas kata konteks valid dan terdistribusi dengan baik.</p></li>
</ul>
</section>
<section id="struktur-skip-gram">
<h3>3. Struktur Skip-Gram<a class="headerlink" href="#struktur-skip-gram" title="Link to this heading">#</a></h3>
<p>Model Skip-Gram menggunakan jaringan saraf untuk mempelajari representasi vektor kata sehingga dapat memprediksi kata-kata konteks di sekitar kata target. Model ini berupaya memahami hubungan antara kata target dan kata-kata konteksnya untuk mengoptimalkan pembelajaran vektor kata.
<img alt="3" src="https://hackmd.io/_uploads/HymY8dH00.png" />
Dalam contoh ini, terdapat 10 kata (T=10) dan 8 kata unik (V=8). Di asumsikan bahwa kata â€œpassesâ€ adalah kata target, dan kata â€œwhoâ€ serta â€œtheâ€ adalah kata konteks. Window size pada contoh tersebut ditetapkan sebesar 1. Ini berarti hanya satu kata di kiri dan satu kata di kanan dari kata target yang dipertimbangkan sebagai kata konteks.
<img alt="4" src="https://hackmd.io/_uploads/r1GsudH0R.png" />
Mengingat ada 8 kata unik (V=8) dan 3 neuron (N=3):
Matriks bobot input (Winput) akan berukuran 8Ã—3, yang berarti setiap kata dalam korpus direpresentasikan oleh vektor 3 dimensi di lapisan tersembunyi.
Matriks bobot output (Woutput) berukuran 3Ã—8, yang menghubungkan lapisan tersembunyi dengan representasi kata-kata konteks yang diprediksi oleh model.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">gensim.models</span> <span class="kn">import</span> <span class="n">Word2Vec</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Word2Vec</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
<p>Code tersebut menjelaskan bagaimana model Word2Vec dapat diimplementasikan menggunakan gensim dengan size=3 untuk jumlah neuron dan window=1 untuk window size.</p>
<section id="forward-propagation-dalam-skip-gram">
<h4>3.1 Forward Propagation dalam Skip-Gram<a class="headerlink" href="#forward-propagation-dalam-skip-gram" title="Link to this heading">#</a></h4>
<p>Forward propagation adalah bagian dari proses pelatihan jaringan saraf, di mana model menghasilkan distribusi probabilitas kata konteks (y_pred) berdasarkan kata target yang diberikan. Matriks embedding kata yang digunakan dalam model Skip-Gram adalah W_input dan W_output. Matriks ini terus dioptimalkan selama pelatihan untuk menangkap hubungan yang lebih baik antara kata-kata.
<img alt="5" src="https://hackmd.io/_uploads/BkihjOHRC.png" />
Lapisan input adalah vektor one-hot berdimensi V, di mana V adalah jumlah kata unik (vocabulary) dalam korpus. Setiap elemen dari vektor ini adalah nol, kecuali satu elemen yang menunjukkan kata target (input). Karena vektor input adalah one-hot, hanya satu baris dari W_input yang diambil, yang sesuai dengan kata target tersebut. Hal ini membuat W_input berfungsi seperti tabel lookup (pencarian) untuk kata target.</p>
</section>
<section id="matriks-embedding-skip-gram">
<h4>3.2 Matriks Embedding Skip-Gram<a class="headerlink" href="#matriks-embedding-skip-gram" title="Link to this heading">#</a></h4>
<p>Dengan memprediksi kata-kata konteks secara efektif, model mampu memperbaiki matriks embedding (Winput dan Woutput) sehingga representasi kata-kata tersebut menjadi lebih bermakna dalam ruang vektor. Winput dan Woutput adalah matriks bobot yang dioptimalkan selama proses pelatihan. Setiap baris dalam matriks ini mewakili sebuah vektor kata (word vector).
<img alt="6" src="https://hackmd.io/_uploads/HyR4fYH00.png" />
Kata â€œpassesâ€ memiliki vektor [0.1, 0.2, 0.7] dan kata â€œshouldâ€ memiliki vektor [-2, 0.2, 0.8]. Karena ukuran vektor ditetapkan menjadi 3 (size=3), kata-kata tersebut direpresentasikan dalam ruang vektor tiga dimensi (3D).
<img alt="7" src="https://hackmd.io/_uploads/B1eLQKrCA.png" />
Optimasi matriks embedding ini bertujuan untuk merepresentasikan kata-kata secara lebih bermakna dalam ruang vektor. Hasilnya, model mampu menangkap hubungan antara kata-kata berdasarkan konteks yang mereka bagikan.</p>
</section>
<section id="hidden-layer-skip-gram">
<h4>3.3 Hidden Layer Skip-Gram<a class="headerlink" href="#hidden-layer-skip-gram" title="Link to this heading">#</a></h4>
<p><img alt="8" src="https://hackmd.io/_uploads/ryP2mKS0R.png" />
Vektor tersembunyi â„ dihasilkan dari perkalian antara matriks embedding kata input (ğ‘Šğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡) dan vektor input satu-hot (ğ‘¥)
$<span class="math notranslate nohighlight">\(h = W_{input}^T \cdot x  \in \mathbb{R}^{N} \tag{12}\)</span>$Hidden Layer h menyimpan representasi vektor yang lebih kompak dari kata target dan berfungsi sebagai input untuk layer output. Dengan optimasi yang tepat, layer ini dapat memberikan representasi yang baik untuk kata-kata dalam ruang vektor, sehingga model dapat belajar untuk memprediksi kata-kata konteks dengan lebih akurat</p>
</section>
<section id="layer-output-softmax-dalam-model-skip-gram">
<h4>3.4 Layer Output Softmax dalam Model Skip-Gram<a class="headerlink" href="#layer-output-softmax-dalam-model-skip-gram" title="Link to this heading">#</a></h4>
<p>Layer output dalam model Skip-Gram menghasilkan distribusi probabilitas ğ‘‰-dimensional dari semua kata unik dalam korpus, berdasarkan kata t. Dalam statistik, probabilitas kondisional dari ğ´ diberikan ğµ dilambangkan sebagai ğ‘(ğ´âˆ£ğµ). Probabilitas kondisional ini dihitung menggunakan fungsi softmax
$<span class="math notranslate nohighlight">\(p(w_{context}|w_{center}) = \frac{exp(W_{output_{(context)}} \cdot h)}{\sum^V_{i=1}exp(W_{output_{(i)}} \cdot h)} \in \mathbb{R}^{1} \tag{13}\)</span><span class="math notranslate nohighlight">\( Di mana ğ‘Šğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡(ğ‘–) adalah vektor baris ke-ğ‘– dari matriks embedding output, dan ğ‘Šğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡(ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡) adalah vektor baris yang sesuai dengan kata konteks. Dan ğ‘‰ adalah jumlah kata unik dalam korpus, dan â„ adalah layer tersembunyi yang memiliki dimensi (ğ‘Ã—1).
Proses ini diulang sebanyak ğ‘‰ kali untuk mendapatkan distribusi probabilitas kondisional untuk setiap kata unik dalam korpus, berdasarkan kata t
\)</span><span class="math notranslate nohighlight">\(\left[ \begin{array}{c} p(w_{1}|w_{center}) \\ p(w_{2}|w_{center}) \\ p(w_{3}|w_{center}) \\ \vdots \\ p(w_{V}|w_{center}) \end{array} \right] = \frac{exp(W_{output} \cdot h)}{\sum^V_{i=1}exp(W_{output_{(i)}} \cdot h)} \in \mathbb{R}^{V}\tag{14}\)</span>$
Dalam persamaan ini, ğ‘Šğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ di penyebut memiliki ukuran ğ‘‰Ã—ğ‘. Mengalikan ğ‘Šğ‘œğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ dengan â„ yang berukuran ğ‘Ã—1 akan menghasilkan vektor produk titik berukuran ğ‘‰Ã—1.
<img alt="9" src="https://hackmd.io/_uploads/BkBdItHCA.png" /></p>
</section>
<section id="backward-propagation-dalam-model-skip-gram">
<h4>3.5 Backward Propagation dalam Model Skip-Gram<a class="headerlink" href="#backward-propagation-dalam-model-skip-gram" title="Link to this heading">#</a></h4>
<p>Backward propagation dalam model Skip-Gram bertujuan untuk menghitung kesalahan prediksi dan memperbarui matriks bobot ğœƒ untuk mengoptimalkan representasi vektor kata.
<img alt="10" src="https://hackmd.io/_uploads/S1K5DKBAA.png" /> Kesalahan prediksi adalah perbedaan antara distribusi probabilitas kata yang dihitung dari layer output softmax (ğ‘¦ğ‘ğ‘Ÿğ‘’ğ‘‘) dan distribusi probabilitas yang sebenarnya (ğ‘¦ğ‘¡ğ‘Ÿğ‘¢ğ‘’) dari kata konteks ke-ğ‘.
<img alt="11" src="https://hackmd.io/_uploads/SJiawKBA0.png" /> Kesalahan prediksi untuk semua kata konteks
ğ¶ dijumlahkan untuk menghitung gradien bobot untuk memperbarui matriks bobot, sesuai dengan persamaan (18) dan (19).
<img alt="12" src="https://hackmd.io/_uploads/Hkdf_tBAR.png" /> Seiring dengan pengoptimalan matriks bobot, kesalahan prediksi untuk semua kata dalam vektor kesalahan prediksi <span class="math notranslate nohighlight">\(\sum_{(c=1)}^C e_c\)</span> akan berkonvergensi menuju 0.</p>
</section>
</section>
<section id="numerical-demonstration">
<h3>4. Numerical demonstration<a class="headerlink" href="#numerical-demonstration" title="Link to this heading">#</a></h3>
<section id="menghitung-hidden-projection-layer-dalam-model-skip-gram">
<h4>4.1 Menghitung Hidden (Projection) Layer dalam Model Skip-Gram<a class="headerlink" href="#menghitung-hidden-projection-layer-dalam-model-skip-gram" title="Link to this heading">#</a></h4>
<p><img alt="13" src="https://hackmd.io/_uploads/ByLzFFHAC.png" /> Dalam contoh ini, kata target yang dipilih adalah â€œpassesâ€. Dengan ukuran jendela yang ditetapkan sebesar 1, maka kata-kata konteks yang relevan adalah â€œtheâ€ dan â€œwhoâ€.</p>
</section>
<section id="softmax-output-layer-dalam-model-skip-gram">
<h4>4.2 Softmax Output Layer dalam Model Skip-Gram<a class="headerlink" href="#softmax-output-layer-dalam-model-skip-gram" title="Link to this heading">#</a></h4>
<p><img alt="14" src="https://hackmd.io/_uploads/rkOsFKBC0.png" />
Fungsi softmax menghitung probabilitas kata konteks berdasarkan kata target. Probabilitas ini mencerminkan seberapa besar kemungkinan kata konteks muncul ketika kata target digunakan. Output dari fungsi softmax akan menghasilkan vektor probabilitas berukuran 1Ã—ğ‘‰ yang menunjukkan kemungkinan setiap kata unik dalam korpus menjadi kata konteks untuk kata target yang diberikan.</p>
</section>
<section id="menjumlahkan-prediction-error-dari-kata-konteks">
<h4>4.3 Menjumlahkan prediction error dari kata konteks<a class="headerlink" href="#menjumlahkan-prediction-error-dari-kata-konteks" title="Link to this heading">#</a></h4>
<p><img alt="15" src="https://hackmd.io/_uploads/ByNroFH0R.png" />
Kesalahan prediksi (prediction errors) untuk setiap kata konteks dihitung sebagai selisih antara probabilitas yang diprediksi (ypred) dan probabilitas yang sebenarnya (ytrue) untuk setiap kata konteks. Setelah menghitung kesalahan untuk masing-masing kata konteks, kesalahan tersebut dijumlahkan. Proses ini bertujuan untuk mendapatkan gradien yang akan digunakan untuk memperbarui bobot matriks (weight matrices) dalam model</p>
</section>
<section id="perhitungan-gradien-untuk-matriks-bobot-input-input">
<h4>4.4 Perhitungan Gradien untuk Matriks Bobot Input (âˆ‡ğ‘Šinput)<a class="headerlink" href="#perhitungan-gradien-untuk-matriks-bobot-input-input" title="Link to this heading">#</a></h4>
<p>Gradien dari matriks bobot input, dinyatakan sebagai  <span class="math notranslate nohighlight">\(\frac{\partial J}{\partial W_{output}}\)</span>, dihitung menggunakan persamaan <span class="math notranslate nohighlight">\(\frac{\partial J}{\partial W_{\text{input}}} = x \cdot \left( W_{\text{output}}^T \sum_{c=1}^{C} e_c \right)\)</span>
Gradien ini digunakan untuk memperbarui bobot input, sehingga model dapat belajar dari kesalahan yang dibuat selama prediksi kata konteks.
<img alt="17" src="https://hackmd.io/_uploads/S1I6ntr0A.png" />
Penambahan <span class="math notranslate nohighlight">\(W_{output}^T \sum^C_{c=1} e_c\)</span> membantu dalam memperhitungkan semua kesalahan yang terjadi dalam jendela konteks, yang memberikan informasi tentang bagaimana bobot harus diperbarui.
Dengan mengalikan <span class="math notranslate nohighlight">\(W_{output}^T \sum^C_{c=1} e_c\)</span>  dengan vektor one-hot encoded ğ‘¥, pembaruan bobot akan fokus hanya pada vektor kata yang sesuai dengan kata t.</p>
</section>
<section id="perhitungan-gradien-untuk-matriks-bobot-output-output">
<h4>4.5 perhitungan gradien untuk matriks bobot output (âˆ‡ğ‘Šoutput)<a class="headerlink" href="#perhitungan-gradien-untuk-matriks-bobot-output-output" title="Link to this heading">#</a></h4>
<p>Gradien dari matriks bobot output, dinyatakan sebagai <span class="math notranslate nohighlight">\(\frac{\partial J}{\partial W_{output}}\)</span>, dihitung menggunakan persamaan <span class="math notranslate nohighlight">\(\frac{\partial J}{\partial W_{\text{output}}} = h \cdot \sum_{c=1}^{C} e_c\)</span>
Gradien ini menunjukkan bagaimana setiap elemen dalam matriks bobot output harus diperbarui berdasarkan kesalahan prediksi yang dihasilkan untuk setiap kata konteks.
<img alt="16" src="https://hackmd.io/_uploads/H1Ff0tSCA.png" />
Pembaruan dilakukan untuk seluruh matriks bobot output, mencerminkan bahwa semua kata dalam kosakata dipertimbangkan dalam konteks kesalahan prediksi. Hal ini penting karena model perlu menyesuaikan hubungan antar kata di seluruh kosakata untuk meningkatkan akurasi prediksi kata konteks yang diharapkan.</p>
</section>
<section id="pembaruan-matriks-bobot-weight-matrices-melalui-backward-propagation">
<h4>4.6 Pembaruan Matriks Bobot (Weight Matrices) melalui Backward Propagation<a class="headerlink" href="#pembaruan-matriks-bobot-weight-matrices-melalui-backward-propagation" title="Link to this heading">#</a></h4>
<p><img alt="18" src="https://hackmd.io/_uploads/Bkdwb9SCA.png" />
<img alt="19" src="https://hackmd.io/_uploads/SkenZqBAR.png" />
Semua bobot dalam ğ‘Šoutput diperbarui, yang berarti setiap vektor kata dalam matriks ini disesuaikan. Namun, dalam ğ‘Šinput, hanya satu baris vektor yang diperbarui, yaitu yang sesuai dengan kata target.
Proses pembaruan matriks bobot adalah langkah penting dalam pelatihan model Skip-Gram. Pembaruan dilakukan secara terpisah untuk ğ‘Šinput dan ğ‘Šoutput, dengan fokus pada satu kata target pada setiap iterasi. Metodologi ini memanfaatkan SGD untuk efisiensi dalam pembaruan bobot, memungkinkan model untuk belajar dari setiap contoh secara individual, sehingga memperbaiki representasi kata di ruang vektor.</p>
</section>
</section>
</section>
<section id="perhitungan-manual-skip-gram">
<h2>Perhitungan Manual Skip-Gram<a class="headerlink" href="#perhitungan-manual-skip-gram" title="Link to this heading">#</a></h2>
<section id="contoh-kata-ibu-beli-sayur-di-pasar">
<h3>Contoh kata = ibu beli sayur di pasar<a class="headerlink" href="#contoh-kata-ibu-beli-sayur-di-pasar" title="Link to this heading">#</a></h3>
<p>Kosakata: [â€œibuâ€, â€œbeliâ€, â€œsayurâ€, â€œdiâ€, â€œpasarâ€]
Indeks: â€œibuâ€: 0, â€œbeliâ€: 1, â€œsayurâ€: 2, â€œdiâ€: 3, â€œpasarâ€: 4
Misalkan bobot awal pada layer embedding (layer pertama) adalah sebagai berikut (dimensi 5 untuk kosakata, dan 5 untuk embedding): $<span class="math notranslate nohighlight">\(W = \begin{bmatrix}
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 &amp; 0.5 \\
0.5 &amp; 0.4 &amp; 0.3 &amp; 0.2 &amp; 0.1 \\
0.2 &amp; 0.3 &amp; 0.4 &amp; 0.5 &amp; 0.6 \\
0.6 &amp; 0.5 &amp; 0.4 &amp; 0.3 &amp; 0.2 \\
0.1 &amp; 0.1 &amp; 0.1 &amp; 0.1 &amp; 0.1
\end{bmatrix}\)</span>$</p>
<section id="iterasi-1">
<h4>Iterasi-1<a class="headerlink" href="#iterasi-1" title="Link to this heading">#</a></h4>
<p>Dari pasangan (â€œbeliâ€,â€ibuâ€):</p>
<ul class="simple">
<li><p>Input (X): One-hot encoding untuk â€œbeliâ€ menjadi [0,1,0,0,0]</p></li>
<li><p>Output (y): One-hot encoding untuk â€œibuâ€ menjadi [1,0,0,0,0]</p></li>
</ul>
<ol class="arabic simple">
<li><p>Hitung Aktivasi Layer Pertama
$<span class="math notranslate nohighlight">\(z = X \cdot W = [0, 1, 0, 0, 0] \cdot W \)</span>$$<span class="math notranslate nohighlight">\(z= [0.5, 0.4, 0.3, 0.2, 0.1]\)</span>$</p></li>
<li><p>Hitung skor output sebelum softmax
$<span class="math notranslate nohighlight">\(z' = z \cdot W' \quad (\text W' \text{ adalah bobot dari layer kedua})\)</span><span class="math notranslate nohighlight">\(Misalkan bobot layer kedua adalah \)</span><span class="math notranslate nohighlight">\(W' =
\begin{bmatrix}
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 &amp; 0.5 \\
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 &amp; 0.5 \\
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 &amp; 0.5 \\
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 &amp; 0.5 \\
0.1 &amp; 0.2 &amp; 0.3 &amp; 0.4 &amp; 0.5 \\
\end{bmatrix}\)</span><span class="math notranslate nohighlight">\( sehingga \)</span><span class="math notranslate nohighlight">\(z' = [0.35, 0.35, 0.35, 0.35, 0.35]\)</span>$</p></li>
<li><p>Hitung Softmax untuk ğ‘§â€²
$<span class="math notranslate nohighlight">\(e^{z'} = [1.419067, 1.419067, 1.419067, 1.419067, 1.419067]\)</span><span class="math notranslate nohighlight">\( \)</span><span class="math notranslate nohighlight">\(\sum_{j} e^{z'_j} = 1.419067 + 1.419067 + 1.419067 + 1.419067 + 1.419067 = 7.095335\)</span><span class="math notranslate nohighlight">\( \)</span><span class="math notranslate nohighlight">\(\text{softmax}(z_i') = \frac{e^{z_i'}}{\sum_{j} e^{z_j'}}\)</span><span class="math notranslate nohighlight">\( Jadi, hasil dari softmax untuk ğ‘§â€² adalah \)</span><span class="math notranslate nohighlight">\(\hat{y} = \text{softmax}(z') = [0.2, 0.2, 0.2, 0.2, 0.2]\)</span>$</p></li>
</ol>
</section>
<section id="iterasi-2">
<h4>Iterasi 2<a class="headerlink" href="#iterasi-2" title="Link to this heading">#</a></h4>
<p>Dari pasangan (â€œbeliâ€,â€sayurâ€):</p>
<ul class="simple">
<li><p>Input (X): One-hot encoding untuk â€œbeliâ€ menjadi [0,1,0,0,0]</p></li>
<li><p>Output (y): One-hot encoding untuk â€œsayurâ€ menjadi [0,0,1,0,0]</p></li>
</ul>
<ol class="arabic simple">
<li><p>Hitung Aktivasi Layer Pertama
$<span class="math notranslate nohighlight">\(z = X \cdot W = [0, 1, 0, 0, 0] \cdot W \)</span>$$<span class="math notranslate nohighlight">\(z= [0.55, 0.45, 0.35, 0.25, 0.15]\)</span>$</p></li>
<li><p>Hitung skor output sebelum softmax
$<span class="math notranslate nohighlight">\(z' = z \cdot W' \quad (\text W' \text{ adalah bobot dari layer kedua})\)</span>$$<span class="math notranslate nohighlight">\(z' = [0.425, 0.425, 0.425, 0.425, 0.425]\)</span>$</p></li>
<li><p>Hitung Softmax untuk ğ‘§â€²
$<span class="math notranslate nohighlight">\(e^{z'} = [1.528,1.528,1.528,1.528,1.528]\)</span><span class="math notranslate nohighlight">\( \)</span><span class="math notranslate nohighlight">\(\sum_{j} e^{z'_j} = 1.528+1.528+1.528+1.528+1.528=7.64\)</span><span class="math notranslate nohighlight">\( \)</span><span class="math notranslate nohighlight">\(\text{softmax}(z_i') = \frac{e^{z_i'}}{\sum_{j} e^{z_j'}}\)</span><span class="math notranslate nohighlight">\( Jadi, hasil dari softmax untuk ğ‘§â€² adalah \)</span><span class="math notranslate nohighlight">\(\hat{y} = \text{softmax}(z') = [0.200,0.200,0.200,0.200,0.200]\)</span>$</p></li>
</ol>
</section>
</section>
</section>
<section id="code-perhitungan">
<h2>Code Perhitungan<a class="headerlink" href="#code-perhitungan" title="Link to this heading">#</a></h2>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># 1. Definisikan kosakata</span>
<span class="n">vocab</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;ibu&quot;</span><span class="p">,</span> <span class="s2">&quot;beli&quot;</span><span class="p">,</span> <span class="s2">&quot;sayur&quot;</span><span class="p">,</span> <span class="s2">&quot;di&quot;</span><span class="p">,</span> <span class="s2">&quot;pasar&quot;</span><span class="p">]</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span>
<span class="n">word_to_index</span> <span class="o">=</span> <span class="p">{</span><span class="n">word</span><span class="p">:</span> <span class="n">idx</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">word</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">vocab</span><span class="p">)}</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Kosakata dan Indeks:&quot;</span><span class="p">,</span> <span class="n">word_to_index</span><span class="p">)</span>

<span class="c1"># 2. Buat pasangan (target, context)</span>
<span class="n">pairs</span> <span class="o">=</span> <span class="p">[</span>
    <span class="p">(</span><span class="s2">&quot;beli&quot;</span><span class="p">,</span> <span class="s2">&quot;ibu&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;beli&quot;</span><span class="p">,</span> <span class="s2">&quot;sayur&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;sayur&quot;</span><span class="p">,</span> <span class="s2">&quot;beli&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;sayur&quot;</span><span class="p">,</span> <span class="s2">&quot;di&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;di&quot;</span><span class="p">,</span> <span class="s2">&quot;sayur&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;di&quot;</span><span class="p">,</span> <span class="s2">&quot;pasar&quot;</span><span class="p">),</span>
    <span class="p">(</span><span class="s2">&quot;pasar&quot;</span><span class="p">,</span> <span class="s2">&quot;di&quot;</span><span class="p">)</span>
<span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Pasangan Target dan Context:&quot;</span><span class="p">,</span> <span class="n">pairs</span><span class="p">)</span>

<span class="c1"># 3. One-Hot Encoding</span>
<span class="k">def</span> <span class="nf">one_hot</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
    <span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">)</span>
    <span class="n">vector</span><span class="p">[</span><span class="n">word_to_index</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">vector</span>

<span class="c1"># Mengubah pasangan menjadi bentuk input dan output</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">one_hot</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="k">for</span> <span class="n">target</span><span class="p">,</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">])</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">one_hot</span><span class="p">(</span><span class="n">context</span><span class="p">)</span> <span class="k">for</span> <span class="n">target</span><span class="p">,</span> <span class="n">context</span> <span class="ow">in</span> <span class="n">pairs</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Input (X):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">X</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Output (y):</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># 4. Membangun Model</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">5</span>  <span class="c1"># Dimensi embedding</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">input_shape</span><span class="o">=</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,),</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)</span>
<span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Model telah dibangun dan disusun.&quot;</span><span class="p">)</span>

<span class="c1"># 5. Melatih Model</span>
<span class="c1"># Meningkatkan epoch dan menampilkan hasil setiap 50 epoch untuk pemantauan</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Menampilkan hasil pelatihan</span>
<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">500</span><span class="p">,</span> <span class="mi">50</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Epoch </span><span class="si">{</span><span class="n">epoch</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2"> - Loss: </span><span class="si">{</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;loss&#39;</span><span class="p">][</span><span class="n">epoch</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">, Accuracy: </span><span class="si">{</span><span class="n">history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">][</span><span class="n">epoch</span><span class="p">]</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># 6. Menampilkan Embedding</span>
<span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">layers</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">embeddings</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="o">.</span><span class="n">get_weights</span><span class="p">()[</span><span class="mi">0</span><span class="p">]</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Embedding:&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">idx</span> <span class="ow">in</span> <span class="n">word_to_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Embedding untuk &#39;</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">&#39;: </span><span class="si">{</span><span class="n">embeddings</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

</pre></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Tugas4\Gambar"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Nama : Isnita Widyur Rahmah</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#nim-220411100048">NIM : 220411100048</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#kelas-if-7a">Kelas : IF 7A</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#word-embedding">Word Embedding</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#skip-gram">Skip-Gram</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#representasi-vektor-kata">1. Representasi Vektor Kata</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#contoh-analogi-dengan-vektor-kata">Contoh Analogi dengan Vektor Kata:</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#konsep-window-size-dalam-model-skip-gram">2. Konsep Window Size dalam model Skip-Gram</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#struktur-skip-gram">3. Struktur Skip-Gram</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-propagation-dalam-skip-gram">3.1 Forward Propagation dalam Skip-Gram</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#matriks-embedding-skip-gram">3.2 Matriks Embedding Skip-Gram</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#hidden-layer-skip-gram">3.3 Hidden Layer Skip-Gram</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-output-softmax-dalam-model-skip-gram">3.4 Layer Output Softmax dalam Model Skip-Gram</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#backward-propagation-dalam-model-skip-gram">3.5 Backward Propagation dalam Model Skip-Gram</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#numerical-demonstration">4. Numerical demonstration</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#menghitung-hidden-projection-layer-dalam-model-skip-gram">4.1 Menghitung Hidden (Projection) Layer dalam Model Skip-Gram</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-output-layer-dalam-model-skip-gram">4.2 Softmax Output Layer dalam Model Skip-Gram</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#menjumlahkan-prediction-error-dari-kata-konteks">4.3 Menjumlahkan prediction error dari kata konteks</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#perhitungan-gradien-untuk-matriks-bobot-input-input">4.4 Perhitungan Gradien untuk Matriks Bobot Input (âˆ‡ğ‘Šinput)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#perhitungan-gradien-untuk-matriks-bobot-output-output">4.5 perhitungan gradien untuk matriks bobot output (âˆ‡ğ‘Šoutput)</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#pembaruan-matriks-bobot-weight-matrices-melalui-backward-propagation">4.6 Pembaruan Matriks Bobot (Weight Matrices) melalui Backward Propagation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#perhitungan-manual-skip-gram">Perhitungan Manual Skip-Gram</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#contoh-kata-ibu-beli-sayur-di-pasar">Contoh kata = ibu beli sayur di pasar</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#iterasi-1">Iterasi-1</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#iterasi-2">Iterasi 2</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#code-perhitungan">Code Perhitungan</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      Â© Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>